# Datadog Monitor Definitions for Redis Rust
#
# Import using: datadogpy or Terraform datadog provider
#
# These monitors cover the key operational metrics for the Redis Rust server.

monitors:
  # Latency Monitors
  - name: "[Redis Rust] High Command Latency (P99 > 10ms)"
    type: "metric alert"
    query: "percentile(last_5m):p99:redis_rust.command.duration{*} > 10"
    message: |
      Command P99 latency is above 10ms.

      Current value: {{value}}ms

      This may indicate:
      - Increased load on the server
      - Slow commands (KEYS, large LRANGE)
      - Resource contention

      @slack-alerts
    options:
      thresholds:
        critical: 10
        warning: 5
      notify_no_data: false
      evaluation_delay: 60
      include_tags: true
    tags:
      - "service:redis-rust"
      - "team:platform"

  - name: "[Redis Rust] Command P50 Latency Elevated"
    type: "metric alert"
    query: "percentile(last_5m):p50:redis_rust.command.duration{*} > 2"
    message: |
      Median command latency is above 2ms.

      Current value: {{value}}ms

      @slack-alerts
    options:
      thresholds:
        critical: 2
        warning: 1
    tags:
      - "service:redis-rust"
      - "team:platform"

  # Connection Monitors
  - name: "[Redis Rust] Connection Spike"
    type: "metric alert"
    query: "avg(last_5m):sum:redis_rust.connections.established{*}.as_count() > 5000"
    message: |
      Active connections exceeded 5000.

      Current count: {{value}}

      This may indicate:
      - Client connection leak
      - Traffic spike
      - Missing connection pooling

      @slack-alerts
    options:
      thresholds:
        critical: 5000
        warning: 3000
    tags:
      - "service:redis-rust"
      - "team:platform"

  - name: "[Redis Rust] Connection Churn"
    type: "metric alert"
    query: "avg(last_5m):sum:redis_rust.connections.established{*}.as_rate() > 1000"
    message: |
      Connection creation rate exceeds 1000/sec.

      High connection churn suggests clients are not reusing connections.

      @slack-alerts
    options:
      thresholds:
        critical: 1000
        warning: 500
    tags:
      - "service:redis-rust"
      - "team:platform"

  # Error Rate Monitors
  - name: "[Redis Rust] High Error Rate"
    type: "metric alert"
    query: |
      100 * sum(last_5m):sum:redis_rust.command.count{status:error}.as_count() /
      sum(last_5m):sum:redis_rust.command.count{*}.as_count() > 1
    message: |
      Error rate exceeds 1%.

      Current rate: {{value}}%

      Check for:
      - Invalid commands
      - Memory pressure
      - Client errors

      @pagerduty-redis
    options:
      thresholds:
        critical: 1
        warning: 0.5
    tags:
      - "service:redis-rust"
      - "team:platform"
      - "severity:high"

  # Persistence Monitors
  - name: "[Redis Rust] Slow Persistence Flush"
    type: "metric alert"
    query: "avg(last_5m):avg:redis_rust.persistence.flush.duration{*} > 1000"
    message: |
      Average persistence flush duration exceeds 1 second.

      Current value: {{value}}ms

      Check:
      - Object store latency
      - Write buffer size
      - Network connectivity

      @slack-alerts
    options:
      thresholds:
        critical: 1000
        warning: 500
    tags:
      - "service:redis-rust"
      - "team:platform"

  - name: "[Redis Rust] Persistence Flush Failures"
    type: "metric alert"
    query: "sum(last_5m):sum:redis_rust.persistence.flush.failures{*}.as_count() > 0"
    message: |
      Persistence flush failures detected!

      Data may not be durable. Immediate investigation required.

      @pagerduty-redis
    options:
      thresholds:
        critical: 0
    tags:
      - "service:redis-rust"
      - "team:platform"
      - "severity:critical"

  # Shard Monitors
  - name: "[Redis Rust] Shard Latency Imbalance"
    type: "metric alert"
    query: |
      max(last_5m):max:redis_rust.shard.operation.duration{*} by {shard_id} -
      min(last_5m):min:redis_rust.shard.operation.duration{*} by {shard_id} > 5
    message: |
      Shard latency imbalance exceeds 5ms.

      Some shards are significantly slower than others.

      Check for:
      - Hot key distribution
      - Resource contention

      @slack-alerts
    options:
      thresholds:
        critical: 5
        warning: 2
    tags:
      - "service:redis-rust"
      - "team:platform"

  # TTL Monitors
  - name: "[Redis Rust] High TTL Eviction Rate"
    type: "metric alert"
    query: "avg(last_5m):sum:redis_rust.ttl.evictions{*}.as_rate() > 10000"
    message: |
      TTL eviction rate exceeds 10,000 keys/second.

      Current rate: {{value}}/s

      This may indicate:
      - Short TTLs
      - Large key churn

      @slack-alerts
    options:
      thresholds:
        critical: 10000
        warning: 5000
    tags:
      - "service:redis-rust"
      - "team:platform"

  # Throughput Monitors
  - name: "[Redis Rust] Low Throughput (Possible Service Issue)"
    type: "metric alert"
    query: "avg(last_10m):sum:redis_rust.command.count{*}.as_rate() < 10"
    message: |
      Command throughput has dropped below 10 ops/sec.

      This may indicate:
      - Service down
      - Network issues
      - No traffic

      @slack-alerts
    options:
      thresholds:
        critical: 10
        warning: 100
      notify_no_data: true
      no_data_timeframe: 10
    tags:
      - "service:redis-rust"
      - "team:platform"
